{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import starmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Sequence Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqfile = '../data/enz_sequence.csv'\n",
    "\n",
    "enzyme_names = []\n",
    "X_raw = []\n",
    "\n",
    "with open(seqfile,'rt') as f:\n",
    "    for lines in f:\n",
    "        val = lines.strip().split(',')\n",
    "        enzyme_names.append(val[0])\n",
    "        X_raw.append(val[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get integer encoded sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAs = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "         'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "\n",
    "AAdict = {aa:i+1 for i,aa in enumerate(AAs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_integer_features(seq):\n",
    "    seq_int = []\n",
    "    for res in seq:\n",
    "        seq_int.append(AAdict[res])\n",
    "    return seq_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_int_enc = list(map(get_integer_features,X_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trim/Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seq,length):\n",
    "    '''pads a variable length sequence into a sequence of length {length} with zeros'''\n",
    "    seq_length = len(seq)\n",
    "    delta = length - seq_length\n",
    "    pre_pad_length = delta//2\n",
    "    post_pad_length = delta - pre_pad_length\n",
    "    \n",
    "    pre_pad = np.zeros(pre_pad_length)\n",
    "    post_pad = np.zeros(post_pad_length)\n",
    "    \n",
    "    seq = np.array(seq)\n",
    "    \n",
    "    seq = np.insert(seq,0,pre_pad)\n",
    "    \n",
    "    seq = np.append(seq,post_pad)\n",
    "    return seq\n",
    "\n",
    "\n",
    "def trim_sequence(seq,length):\n",
    "    '''trims a variable length sequence into a sequence of equal length'''\n",
    "    seq_length = len(seq)\n",
    "    delta = seq_length - length\n",
    "    pre_trim_length = delta//2\n",
    "    post_trim_length = delta - pre_trim_length\n",
    "    \n",
    "    seq = np.array(seq)\n",
    "    \n",
    "    return seq[pre_trim_length:seq_length-post_trim_length]\n",
    "\n",
    "\n",
    "def process_sequence(seq,length):\n",
    "    '''modifies a sequence of variable length into an user-defined length'''\n",
    "    seq_length = len(seq)\n",
    "    if seq_length == length:\n",
    "        return np.array(seq)\n",
    "    \n",
    "    elif seq_length<length:\n",
    "        return pad_sequence(seq,length)\n",
    "    \n",
    "    elif seq_length>length:\n",
    "        return trim_sequence(seq,length)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get median length of all seq\n",
    "all_seq_length = list(map(len,X_int_enc))\n",
    "median_len = np.median(all_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "argument_iter = zip(X_int_enc,[int(median_len) for i in range(len(X_int_enc))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_int_samelen = list(starmap(process_sequence,argument_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_int_enc = np.array(X_int_samelen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6033, 408)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_int_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.fit(X_int_enc)\n",
    "\n",
    "X = ohe.transform(X_int_enc).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelfile = '../data/enz_labels.csv'\n",
    "\n",
    "enzyme_namesy = []\n",
    "y_raw = []\n",
    "\n",
    "with open(labelfile,'rt') as f:\n",
    "    for lines in f:\n",
    "        val = lines.strip().split(',')\n",
    "        enzyme_namesy.append(val[0])\n",
    "        y_raw.append(int(val[1]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert enzyme_names == enzyme_namesy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enzyme_names = np.array(enzyme_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0_idx = np.argwhere(y==0).flatten()\n",
    "y1_idx = np.argwhere(y==1).flatten()\n",
    "\n",
    "train0_idx = np.random.choice(y0_idx,size=int(0.75*len(y0_idx)),replace=False)\n",
    "train1_idx = np.random.choice(y1_idx,size=int(0.45*len(y1_idx)),replace=False)\n",
    "\n",
    "valid0_idx = np.array([i for i in y0_idx if i not in train0_idx])\n",
    "valid1_idx = np.array([i for i in y1_idx if i not in train1_idx])\n",
    "\n",
    "train_idx = np.append(train0_idx,train1_idx)\n",
    "valid_idx = np.append(valid0_idx,valid1_idx)\n",
    "\n",
    "np.random.shuffle(train_idx)\n",
    "np.random.shuffle(valid_idx)\n",
    "\n",
    "X_train,X_valid = X[train_idx],X[valid_idx]\n",
    "y_train,y_valid = y[train_idx],y[valid_idx]\n",
    "enz_train,enz_valid = enzyme_names[train_idx],enzyme_names[valid_idx]\n",
    "\n",
    "assert len(enz_train) + len(enz_valid) == len(enzyme_names)\n",
    "assert len(X_train) + len(X_valid) == len(X)\n",
    "assert len(y_train) + len(y_valid) == len(y)\n",
    "\n",
    "X_train = X_train.reshape(-1,1,X.shape[1]).astype('float32')\n",
    "X_valid = X_valid.reshape(-1,1,X.shape[1]).astype('float32')\n",
    "y_train = y_train.reshape(-1,1).astype('float32')\n",
    "y_valid = y_valid.reshape(-1,1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = []\n",
    "for i in range(len(X_train)):\n",
    "    trainset.append([X_train[i],y_train[i]])\n",
    "\n",
    "validset = []\n",
    "for i in range(len(X_valid)):\n",
    "    validset.append([X_valid[i],y_valid[i]])\n",
    "    \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=500,\n",
    "                                          shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_encoded_len = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True,dropout=0.0)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        hidden_state = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, (hidden,hidden_state))\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.randn(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden\n",
    "\n",
    "n_hidden = 50\n",
    "n_categories = 1\n",
    "rnn = RNN(seq_encoded_len, 1, n_hidden, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "n_epochs = 5\n",
    "lr=0.001\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/5............. Loss: 0.699357\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,n_epochs+1):\n",
    "    \n",
    "    \n",
    "    for i,data in enumerate(trainloader,0):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ = data[0]\n",
    "        label = data[1]\n",
    "\n",
    "\n",
    "        hidden = rnn.init_hidden(input_.shape[0])\n",
    "        \n",
    "\n",
    "        ypred,next_hidden = rnn(input_)\n",
    "        \n",
    "        \n",
    "        loss = criterion(ypred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.6f}\".format(loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "for i,data in enumerate(validloader):\n",
    "    inputs, labels = data\n",
    "    \n",
    "    all_labels.extend(labels.flatten().detach().numpy())\n",
    "    \n",
    "    \n",
    "    outputs,_ = rnn(inputs)    \n",
    "    num_correct += (outputs.round() == labels).sum()\n",
    "    \n",
    "    all_preds.extend(outputs.round().flatten().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5639666919000756"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct.item()/(len(validset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.], dtype=float32), array([ 564, 2078]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_labels,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.], dtype=float32), array([1096, 1546]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_preds,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6821192052980133"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(all_labels,all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23175182481751824"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(all_labels,all_preds,pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.450354609929078"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(all_labels,all_preds,pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
